{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbcbb51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from utils import *\n",
    "\n",
    "# Outlined in Johnson et al.\n",
    "from transformer import TransformerNet\n",
    "\n",
    "# pretrained VGG to extract features form relu_1_2, relu_2_2, relu_3_3 and relu_4_3\n",
    "from vgg import PerceptualLossNet\n",
    "\n",
    "from torch.optim import Adam\n",
    "import time\n",
    "from PIL import Image\n",
    "import os\n",
    "import pickle\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "TRAIN_IMAGE_SIZE = 256\n",
    "DATASET_PATH = \"./data\"\n",
    "NUM_EPOCHS = 1\n",
    "STYLE_IMAGE_PATH = \"./style_image.jpeg\"\n",
    "CONTENT_IMAGE_PATH = \"content.jpeg\"\n",
    "BATCH_SIZE = 4\n",
    "CONTENT_WEIGHT = 1e0\n",
    "STYLE_WEIGHT = 4e5\n",
    "TV_WEIGHT = 1e-6\n",
    "LR = 0.001\n",
    "SAVE_MODEL_PATH = \"./checkpoints\"\n",
    "SAVE_IMAGE_PATH = \"./image_outputs\"\n",
    "CHECKPOINT_FREQ = 150\n",
    "LOG_FREQ = 50\n",
    "\n",
    "# Setting the seed value for reproducibility\n",
    "SEED = 42\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "\n",
    "class StyleTransfer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_epochs=NUM_EPOCHS,\n",
    "        image_size=TRAIN_IMAGE_SIZE,\n",
    "        dataset_path=DATASET_PATH,\n",
    "        style_image_path=STYLE_IMAGE_PATH,\n",
    "        content_image_path=CONTENT_IMAGE_PATH,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        style_weight=STYLE_WEIGHT,\n",
    "        content_weight=CONTENT_WEIGHT,\n",
    "        tv_weight=TV_WEIGHT,\n",
    "        log_freq=LOG_FREQ,\n",
    "        checkpoint_freq=CHECKPOINT_FREQ,\n",
    "        lr=LR,\n",
    "        save_model_path=SAVE_MODEL_PATH,\n",
    "        save_image_path=SAVE_IMAGE_PATH\n",
    "    ):\n",
    "        self.epochs = num_epochs\n",
    "        self.image_size = image_size\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.batch_size = batch_size\n",
    "        self.dataset_path = dataset_path\n",
    "        self.style_image_path = style_image_path\n",
    "        self.content_image_path = content_image_path\n",
    "        self.content_weight = content_weight\n",
    "        self.style_weight = style_weight\n",
    "        self.tv_weight = tv_weight\n",
    "        self.lr = lr\n",
    "        self.log_freq = log_freq\n",
    "        self.checkpoint_freq = checkpoint_freq\n",
    "        self.save_model_path = save_model_path\n",
    "        self.save_image_path = save_image_path\n",
    "\n",
    "        # load data\n",
    "        print(\"Loading Data...\")\n",
    "        self.train_loader, self.val_loader = get_training_data_loader(dataset_path, image_size, batch_size)\n",
    "        print(\"Data Loaded Successfully \\n\")\n",
    "        \n",
    "        # instantiate networks\n",
    "        self.transformer_net = TransformerNet().train().to(self.device)\n",
    "        self.perceptual_loss_net = PerceptualLossNet(requires_grad=False).to(self.device)\n",
    "\n",
    "        self.optimizer = Adam(self.transformer_net.parameters())\n",
    "\n",
    "        # Compute Gram matrices for the style image\n",
    "        style_img = prepare_img(style_image_path, self.device, batch_size=batch_size)\n",
    "        style_img_set_of_feature_maps = self.perceptual_loss_net(style_img)\n",
    "        self.target_style_representation = [gram_matrix(x) for x in style_img_set_of_feature_maps]\n",
    "        \n",
    "        # This image is used to keep track of the subjective performance over the iteration\n",
    "        # sotred in the directory \"save_image_path\" after every log_iter iterations\n",
    "        self.test_image = prepare_img(content_image_path, self.device)\n",
    "        \n",
    "        self.mse_loss = torch.nn.MSELoss(reduction='mean')\n",
    "\n",
    "        self.best_val_loss = None\n",
    "        \n",
    "        self.history = {\n",
    "            'content_loss_t': [],\n",
    "            'style_loss_t': [],\n",
    "            'tv_loss_t': [],\n",
    "            'total_loss_t': [],\n",
    "            'total_loss_v' : []\n",
    "        }\n",
    "    \n",
    "    \n",
    "    def train(self):\n",
    "        print(\"Training Started...\\n\")\n",
    "        \n",
    "        ts = time.time()\n",
    "        \n",
    "        acc_content_loss, acc_style_loss, acc_tv_loss = [0., 0., 0.]\n",
    "        \n",
    "        for epoch in range(self.epochs):\n",
    "            for batch_id, (content_batch, _) in enumerate(self.train_loader):\n",
    "                # get ouput of transform_net\n",
    "                content_batch = content_batch.to(self.device)\n",
    "                stylized_batch = self.transformer_net(content_batch)\n",
    "\n",
    "                # feed batch of style and content images to the vgg net\n",
    "                content_batch_set_of_feature_maps = self.perceptual_loss_net(content_batch)\n",
    "                stylized_batch_set_of_feature_maps = self.perceptual_loss_net(stylized_batch)\n",
    "\n",
    "                # compute content loss\n",
    "                target_content_representation = content_batch_set_of_feature_maps.relu2_2\n",
    "                current_content_representation = stylized_batch_set_of_feature_maps.relu2_2\n",
    "                content_loss = self.content_weight * self.mse_loss(target_content_representation, current_content_representation)\n",
    "                acc_content_loss += content_loss.item()\n",
    "                \n",
    "                # compute gram matrices and style loss\n",
    "                style_loss = 0.0\n",
    "                current_style_representation = [gram_matrix(x) for x in stylized_batch_set_of_feature_maps]\n",
    "                for gram_gt, gram_hat in zip(self.target_style_representation, current_style_representation):\n",
    "                    style_loss += self.mse_loss(gram_gt, gram_hat)\n",
    "                style_loss /= len(self.target_style_representation)\n",
    "                style_loss *= self.style_weight\n",
    "                acc_style_loss += style_loss.item()\n",
    "\n",
    "                # compute tv loss\n",
    "                tv_loss = self.tv_weight * total_variation(stylized_batch)\n",
    "                acc_tv_loss += tv_loss.item()\n",
    "\n",
    "                # backprop\n",
    "                total_loss = content_loss + style_loss + tv_loss\n",
    "                total_loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                \n",
    "                if (batch_id + 1) % self.log_freq == 0:\n",
    "                    with torch.no_grad():\n",
    "                        self.history['content_loss_t'].append(acc_content_loss / self.log_freq)\n",
    "                        self.history['style_loss_t'].append(acc_style_loss / self.log_freq)\n",
    "                        self.history['tv_loss_t'].append(acc_tv_loss / self.log_freq)\n",
    "                        self.history['total_loss_t'].append((acc_content_loss + acc_style_loss + acc_tv_loss) / self.log_freq)\n",
    "\n",
    "                        self.transformer_net.eval()\n",
    "                        stylized_test = self.transformer_net(self.test_image).cpu().numpy()[0]\n",
    "                        val_loss = self.val_loss()\n",
    "                        self.transformer_net.train()\n",
    "                        stylized = post_process_image(stylized_test)\n",
    "                        stylized_image = Image.fromarray(stylized)\n",
    "\n",
    "                        stylized_image.save(os.path.join(self.save_image_path, f\"iter-{batch_id + 1}.jpeg\"))\n",
    "                        \n",
    "                        \n",
    "                        \n",
    "                        if self.best_val_loss is None or val_loss < self.best_val_loss:\n",
    "                            self.best_val_loss = val_loss\n",
    "                            torch.save(self.transformer_net.state_dict(), \"best_model.pth\")\n",
    "                            \n",
    "                        print(f'Iter : [{batch_id + 1}/{len(self.train_loader)}]')\n",
    "                        print('---------------------\\n')\n",
    "                        print(f'Time Elapsed : {(time.time() - ts) / 60:.2f} min)')\n",
    "                        print('Training Loss :')\n",
    "                        print(f'\\tContent Loss : {acc_content_loss / self.log_freq}')\n",
    "                        print(f'\\tStyle Loss : {acc_style_loss / self.log_freq}')\n",
    "                        print(f'\\tTV Loss : {acc_tv_loss / self.log_freq}')\n",
    "                        print(f'\\tTotal Loss : {(acc_content_loss + acc_style_loss + acc_tv_loss) / self.log_freq}')\n",
    "                        print(f'Validation Loss : {val_loss}\\n\\n')\n",
    "                    \n",
    "                        \n",
    "                        acc_content_loss, acc_style_loss, acc_tv_loss = [0., 0., 0.]\n",
    "\n",
    "                \n",
    "                if (batch_id + 1) % self.checkpoint_freq == 0:\n",
    "                    torch.save(self.transformer_net.state_dict(),\n",
    "                               os.path.join(self.save_model_path, f\"iter-{batch_id + 1}.pth\"))\n",
    "\n",
    "\n",
    "                            \n",
    "    def val_loss(self):\n",
    "        val_loss = 0.0\n",
    "        for batch_id, (content_batch, _) in enumerate(self.val_loader):\n",
    "            content_batch = content_batch.to(self.device)\n",
    "            stylized_batch = self.transformer_net(content_batch)\n",
    "            \n",
    "            content_batch_set_of_feature_maps = self.perceptual_loss_net(content_batch)\n",
    "            stylized_batch_set_of_feature_maps = self.perceptual_loss_net(stylized_batch)\n",
    "            \n",
    "            target_content_representation = content_batch_set_of_feature_maps.relu2_2\n",
    "            current_content_representation = stylized_batch_set_of_feature_maps.relu2_2\n",
    "            content_loss = self.content_weight * self.mse_loss(target_content_representation, current_content_representation)\n",
    "\n",
    "            style_loss = 0.0\n",
    "            current_style_representation = [gram_matrix(x) for x in stylized_batch_set_of_feature_maps]\n",
    "            for gram_gt, gram_hat in zip(self.target_style_representation, current_style_representation):\n",
    "                style_loss += self.mse_loss(gram_gt, gram_hat)\n",
    "            style_loss /= len(self.target_style_representation)\n",
    "            style_loss *= self.style_weight\n",
    "            \n",
    "            tv_loss = self.tv_weight * total_variation(stylized_batch)\n",
    "\n",
    "            val_loss += (content_loss + style_loss + tv_loss).item()\n",
    "            \n",
    "        val_loss /= len(self.val_loader)\n",
    "        self.history['total_loss_v'].append(val_loss)\n",
    "                \n",
    "        return val_loss\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    style_transfer = StyleTransfer()\n",
    "    style_transfer.train()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
